---
title: "Practical Machine Learning Course Project"
author: "Evgeny Gorelov"
date: "4/1/2019"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Preparing the data
In order to make the analysis reproducable,
the script starts from files download, and the random number seed is set:
```{r load}
library(caret)
set.seed(123)
if (!file.exists("pml-training.csv"))
   download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", "pml-training.csv")
if (!file.exists("pml-testing.csv"))
   download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", "pml-testing.csv")
trSet<-read.csv("pml-training.csv")
cases<-read.csv("pml-testing.csv")
```
The dataset contains number of factor variables of numeric nature,
and some non-informative for our purpoces variables: **user_name**, **cvtd_timestamp**.
The factor variables are converted to numeric format, extra variables removed:
```{r convert}
trSet<-trSet[,!(names(trSet)%in% c("user_name","cvtd_timestamp","new_window"))]
cases<-cases[,!(names(cases)%in% c("user_name","cvtd_timestamp","new_window"))]
classeBak<-trSet$classe
trSet<-data.frame(lapply(trSet, function(x) as.numeric(as.character(x))))
trSet$classe<-classeBak
cases<-data.frame(lapply(cases, function(x) as.numeric(as.character(x))))
```
Since the dataset contains many variables with *NA* values dominating,
these predictors (having overall *NA* quote more than 97%) will be omitted, 
and the rest entries are free from NA values. In order to keep the model consistent,
the same procedure is performed on the target **cases** data:
```{r clean, echo=FALSE}
trSetClean<-trSet[,sapply(trSet,function(col){sum(is.na(col))<0.97*length(col)})]
sum(complete.cases(trSetClean)); dim(trSetClean)
casesClean<-cases[,sapply(trSet,function(col){sum(is.na(col))<0.97*length(col)})]
dim(casesClean)
```

## Data analysis
### Data partitioning
Sinse the data set is quite big (19622 observations), we can perform the validation procedure.
The testing set is splitted: 30% to validation set,
and rest to training (70%) and testing (30%).
In order to demonstrate cross-validation procedure, two additional splittings
to training/testing sets are performed:
```{r split, echo=FALSE}
inValidat<-createDataPartition(y=trSetClean$classe, p=0.3, list=FALSE)
validation<-trSetClean[inValidat,]; Build<-trSetClean[-inValidat,]
inTrain<-createDataPartition(y=Build$classe, p=0.7, list=FALSE)
training<-Build[inTrain,]; testing<-Build[-inTrain,]
inTrain<-createDataPartition(y=Build$classe, p=0.7, list=FALSE)
training2<-Build[inTrain,]; testing2<-Build[-inTrain,]
inTrain<-createDataPartition(y=Build$classe, p=0.7, list=FALSE)
training3<-Build[inTrain,]; testing3<-Build[-inTrain,]
```
### Principal component construction
Since number of predictors (56) is still high, and, according to origin of the data,
some of them could be highly correlated, a Principle Component-based preprocessing is used.
In order to capture 70 percent of the variance, it is enough to use 9 to 11 principal components,
depending on selection of the training set:
```{r preprocess, echo=FALSE}
preProc<-preProcess(training[,-57], method="pca", thresh=0.7)
preProc$numComp
trainPC<-cbind(predict(preProc, training[,-57]), classe=training[,57]); testPC<-predict(preProc, testing[,-57])
preProc2<-preProcess(training2[,-57], method="pca", thresh=0.7)
preProc2$numComp
trainPC2<-cbind(predict(preProc2, training2[,-57]), classe=training[,57]); testPC2<-predict(preProc2, testing2[,-57])
preProc3<-preProcess(training3[,-57], method="pca", thresh=0.7)
preProc3$numComp
trainPC3<-cbind(predict(preProc3, training3[,-57]), classe=training[,57]); testPC3<-predict(preProc3, testing3[,-57])
validationPC<-predict(preProc, validation[,-57]);validationPC2<-predict(preProc2, validation[,-57]);validationPC3<-predict(preProc3, validation[,-57])
casesPC<-predict(preProc, casesClean[,-57]);casesPC2<-predict(preProc2, casesClean[,-57]);casesPC3<-predict(preProc3, casesClean[,-57])
```
### Models fit and error analysis
Among the variety of models tried for the task, the two demonstrated reasonable prediction accuracy (above 70%)
and being able to run on 8GB RAM machine: **Random Forest** and **gradient boosted** models. 
In order to perform cross-validation and error estimation, each model is fitted three times,
using *training*, *training2*, and *training3* sets for fitting, 
and *testing*, *testing2*, and *testing3* for estimating the out-of-sample error by cross-validation.
```{r modelsCrossVal, echo=FALSE}
mod1a<-train(classe~., method="rf", data=trainPC)
mod2a<-train(classe~., method="gbm", data=trainPC, verbose=FALSE)
mod1b<-train(classe~., method="rf", data=trainPC2)
mod2b<-train(classe~., method="gbm", data=trainPC2, verbose=FALSE)
mod1c<-train(classe~., method="rf", data=trainPC3)
mod2c<-train(classe~., method="gbm", data=trainPC3, verbose=FALSE)
errA<-confusionMatrix(testing$classe, predict(mod1a, testPC))$overall[1]
errB<-confusionMatrix(testing2$classe, predict(mod1b, testPC2))$overall[1]
errC<-confusionMatrix(testing3$classe, predict(mod1c, testPC3))$overall[1]
1.0-mean(c(errA, errB, errC))
errA<-confusionMatrix(testing$classe, predict(mod2a, testPC))$overall[1]
errB<-confusionMatrix(testing2$classe, predict(mod2b, testPC2))$overall[1]
errC<-confusionMatrix(testing3$classe, predict(mod2c, testPC3))$overall[1]
1.0-mean(c(errA, errB, errC))
```
The error estimated by cross-validation is  5% for  **rf** method, and 22% for **gbm**.
However, desining data partitioning the **validation** independent set have been generated,
so true out-of-sample error rate is
```{r OOSerror, echo=FALSE}
errA<-confusionMatrix(validation$classe, predict(mod1a, validationPC))$overall[1]
errB<-confusionMatrix(validation$classe, predict(mod1b, validationPC2))$overall[1]
errC<-confusionMatrix(validation$classe, predict(mod1c, validationPC3))$overall[1]
1.0-mean(c(errA, errB, errC))
errA<-confusionMatrix(validation$classe, predict(mod2a, validationPC))$overall[1]
errB<-confusionMatrix(validation$classe, predict(mod2b, validationPC2))$overall[1]
errC<-confusionMatrix(validation$classe, predict(mod2c, validationPC3))$overall[1]
1.0-mean(c(errA, errB, errC))
```
The out-of-sample error is  5% for  **rf** method, and 23% for **gbm**,
it is very close to cross-validation estimations. The confidence intervals for accuracy associated with in-sample error for two type of models (**rf** and **gbm**) and three train/test partitionings for each model are shown below:
```{r AccuPlot, echo=FALSE}
resmp<-resamples(list(RandomForest1=mod1a, RandomForest2=mod1b, RandomForest3=mod1c, 
                      GBM1=mod2a, GBM2=mod2b, GBM3=mod2c))
trellis.par.set(caretTheme())
dotplot(resmp, metric = "Accuracy")
```

### Model stacking
In order to maintain high prediction accuracy, the model stacking method is implemented.
```{r modelsStack, echo=FALSE}
#pred1test<-predict(mod1, testPC)
#pred2test<-predict(mod2, testPC)

predDF<-data.frame(pred1=predict(mod1a, testPC),pred2=predict(mod1b, testPC2),pred3=predict(mod1c, testPC3),pred4=predict(mod2a, testPC),pred5=predict(mod2b, testPC2),pred6=predict(mod2c, testPC3),classe=testing$classe)

#predDF<-data.frame(pred1=pred1test,pred2=pred2test, classe=testing$classe)

combModFit<-train(classe~., method="rf", data=predDF)

predVDF<-data.frame(pred1=predict(mod1a, validationPC),pred2=predict(mod1b, validationPC2),pred3=predict(mod1c, validationPC3),pred4=predict(mod2a, validationPC),pred5=predict(mod2b, validationPC2),pred6=predict(mod2c, validationPC3))

combPredV<-predict(combModFit, predVDF)
1.0-confusionMatrix(validation$classe, combPredV)$overall[1]
```

The out-of-sample error rate for stacked model is 4%, that is better than any individual model used in the stacking.

Applying the resulting stacked model to the target Quiz **cases** data set, we obtain
```{r QuizCases, echo=FALSE}
cases6<-data.frame(pred1=predict(mod1a, casesPC),pred2=predict(mod1b, casesPC2),pred3=predict(mod1c, casesPC3),pred4=predict(mod2a, casesPC),pred5=predict(mod2b, casesPC2),pred6=predict(mod2c, casesPC3))
data.frame(predict(combModFit, cases6))
```
